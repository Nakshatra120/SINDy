{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea5a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import lstsq\n",
    "from itertools import combinations_with_replacement\n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "410ae9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz(t, x, sigma=10.0, rho=28.0, beta=8/3):\n",
    "    X, Y, Z = x\n",
    "    return [sigma*(Y - X), X*(rho - Z) - Y, X*Y - beta*Z]\n",
    "\n",
    "# simulate\n",
    "dt = 0.002\n",
    "T  = 20.0\n",
    "t_span = (0, T)\n",
    "t_eval = np.arange(0, T, dt)\n",
    "x0 = [ -8.0, 8.0, 27.0 ]   # initial condition\n",
    "\n",
    "sol = solve_ivp(lorenz, t_span, x0, t_eval=t_eval, rtol=1e-10, atol=1e-12)\n",
    "t = sol.t\n",
    "X = sol.y.T                    # shape (N, 3), columns = [x, y, z]\n",
    "\n",
    "# We’ll use this only to create data; pretend that we don’t know the equations afterward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5aa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding some noise to add robustness\n",
    "\n",
    "noise_level = 0.0            # e.g., 0.01 for 1% noise\n",
    "X_noisy = X + noise_level*np.std(X, axis=0)*np.random.randn(*X.shape)\n",
    "\n",
    "X_data = X_noisy\n",
    "# Use X_data = X_noisy if you add noise; otherwise X_data = X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623344a",
   "metadata": {},
   "source": [
    "## Estimating now the x derivatives $\\dot{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074764bf",
   "metadata": {},
   "source": [
    "SINDy assumes you have $\\dot{x}$(t). With noise-free data you can finite-difference; with realistic data, a Savitzky–Golay (SG) filter is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b573d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def estimate_derivative_savgol(X, dt, window_frac=0.05, polyorder=3):\n",
    "#     N = len(X)\n",
    "#     win = max(5, int(window_frac*N) | 1)  # odd, at least 5\n",
    "#     win = min(win, N-(1-N%2))             # ensure <= N and odd\n",
    "#     dXdt = savgol_filter(X, window_length=win, polyorder=polyorder,\n",
    "#                          deriv=1, delta=dt, axis=0, mode='interp')\n",
    "#     return dXdt\n",
    "\n",
    "# dXdt = estimate_derivative_savgol(X_data, dt)\n",
    "\n",
    "dXdt = np.array([lorenz(ti, xi) for ti, xi in zip(t, X_data)]) #if not using the Savgol filter, when testing system without noise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bb4193",
   "metadata": {},
   "source": [
    "$$ \\text{Build a polynomial library } \\Theta(X) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a178570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10),\n",
       " ['1', 'x1', 'x2', 'x3', 'x1^2', 'x1*x2', 'x1*x3', 'x2^2', 'x2*x3', 'x3^2'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building a polynomial library\n",
    "## Include constant, linear, and quadratic terms (sufficient for Lorenz).\n",
    "\n",
    "def monomial_exponents(d, degree):\n",
    "    # all exponent tuples e with sum(e) in 1..degree\n",
    "    exps = []\n",
    "    for deg in range(1, degree+1):\n",
    "        for idxs in combinations_with_replacement(range(d), deg):\n",
    "            e = np.zeros(d, dtype=int)\n",
    "            for k in idxs:\n",
    "                e[k] += 1\n",
    "            exps.append(tuple(e))\n",
    "    return exps\n",
    "\n",
    "def build_library(X, degree=2, include_bias=True, names=None):\n",
    "    \"\"\"\n",
    "    X: (N,d) data matrix\n",
    "    returns Theta: (N,p), feature_names: list[str], exponents: list[tuple]\n",
    "    \"\"\"\n",
    "    N, d = X.shape\n",
    "    features = []\n",
    "    names_out = []\n",
    "    exps = []\n",
    "\n",
    "    if include_bias:\n",
    "        features.append(np.ones((N,1)))\n",
    "        names_out.append(\"1\")\n",
    "        exps.append(tuple([0]*d))\n",
    "\n",
    "    # linear terms\n",
    "    for j in range(d):\n",
    "        features.append(X[:, [j]])\n",
    "        names_out.append(f\"x{j+1}\")\n",
    "        e = [0]*d; e[j]=1\n",
    "        exps.append(tuple(e))\n",
    "\n",
    "    # higher-order terms\n",
    "    for e in monomial_exponents(d, degree):\n",
    "        if sum(e) == 1:          # already added linear\n",
    "            continue\n",
    "        term = np.prod([X[:, [j]]**e[j] for j in range(d)], axis=0)\n",
    "        features.append(term)\n",
    "        # build a readable name\n",
    "        name = []\n",
    "        for j, p in enumerate(e):\n",
    "            if p>0:\n",
    "                name.append(f\"x{j+1}^{p}\" if p>1 else f\"x{j+1}\")\n",
    "        names_out.append(\"\".join(name).replace(\"x1x2\",\"x1*x2\").replace(\"x1x3\",\"x1*x3\").replace(\"x2x3\",\"x2*x3\"))\n",
    "        exps.append(e)\n",
    "\n",
    "    Theta = np.hstack(features)\n",
    "    return Theta, names_out, exps\n",
    "\n",
    "Theta_raw, feat_names, exps = build_library(X_data, degree=2, include_bias=True)\n",
    "Theta_raw.shape, feat_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c041c8",
   "metadata": {},
   "source": [
    "$$ \\text{Column scaling (important for thresholding)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "630dfb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale each column to unit 2-norm so a single threshold λ is meaningful\n",
    "def scale_columns(A):\n",
    "    s = np.linalg.norm(A, axis=0)\n",
    "    s[s==0] = 1.0\n",
    "    return A / s, s\n",
    "\n",
    "Theta, col_scales = scale_columns(Theta_raw)\n",
    "\n",
    "# When you recover coefficients, you’ll unscale by dividing by col_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8465b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84765.55572213189\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(Theta_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a34f9806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.17972447e-05 -9.43779573e-05  9.43779573e-05 ...  7.55023659e-04\n",
      "   2.54820485e-03  8.60019136e-03]\n",
      " [ 1.17972447e-05 -9.06442282e-05  9.39798077e-05 ...  7.48666702e-04\n",
      "   2.51222299e-03  8.43000541e-03]\n",
      " [ 1.17972447e-05 -8.69927002e-05  9.35437962e-05 ...  7.41736063e-04\n",
      "   2.47610252e-03  8.26585627e-03]\n",
      " ...\n",
      " [ 1.17972447e-05  7.82388648e-07  3.30044092e-06 ...  9.23343591e-07\n",
      "   4.83907459e-05  2.53607033e-03]\n",
      " [ 1.17972447e-05  8.32395941e-07  3.31542864e-06 ...  9.31748675e-07\n",
      "   4.83520555e-05  2.50917585e-03]\n",
      " [ 1.17972447e-05  8.81724021e-07  3.33184694e-06 ...  9.40999731e-07\n",
      "   4.83331705e-05  2.48256752e-03]]\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "new_theta = (Theta_raw/np.linalg.norm(Theta_raw))\n",
    "print(new_theta)\n",
    "print(new_theta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d093006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  100.           794.39613016   891.67591552  2523.0892007\n",
      "  9237.52572258 10003.61076321 24270.679128   13086.14574351\n",
      " 23173.73083554 75462.95992238]\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "print(np.linalg.norm(Theta_raw, axis = 0))\n",
    "print((np.linalg.norm(Theta_raw, axis = 0)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edcef7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e-02 -1.00705425e-02  8.97186956e-03 ...  4.89066844e-03\n",
      "   9.32089880e-03  9.66036849e-03]\n",
      " [ 1.00000000e-02 -9.67213722e-03  8.93402018e-03 ...  4.84949123e-03\n",
      "   9.18928330e-03  9.46920309e-03]\n",
      " [ 1.00000000e-02 -9.28250314e-03  8.89257154e-03 ...  4.80459799e-03\n",
      "   9.05716078e-03  9.28481868e-03]\n",
      " ...\n",
      " [ 1.00000000e-02  8.34843047e-05  3.13750437e-04 ...  5.98096140e-06\n",
      "   1.77005097e-04  2.84870102e-03]\n",
      " [ 1.00000000e-02  8.88203024e-05  3.15175218e-04 ...  6.03540537e-06\n",
      "   1.76863574e-04  2.81849116e-03]\n",
      " [ 1.00000000e-02  9.40838252e-05  3.16735994e-04 ...  6.09532911e-06\n",
      "   1.76794496e-04  2.78860272e-03]]\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(Theta_raw/np.linalg.norm(Theta_raw, axis = 0))\n",
    "print((Theta_raw/np.linalg.norm(Theta_raw, axis = 0)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0813956",
   "metadata": {},
   "source": [
    "$$ \\text{Sparse regression (STLSQ)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0afeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequentially threshold small coefficients to zero, then refit on the remaining terms. \n",
    "# Do this per state component.def stlsq(Theta, dXdt, lam=0.1, max_iter=20, tol=1e-8):\n",
    "\n",
    "def stlsq(Theta, dXdt, lam=0.1, max_iter=20, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Theta: (N,p), dXdt: (N,d)\n",
    "    returns Xi: (p,d) sparse coefficient matrix\n",
    "    \"\"\"\n",
    "    N, p = Theta.shape\n",
    "    _, d = dXdt.shape\n",
    "    Xi, _, _, _ = lstsq(Theta, dXdt, rcond=None)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        Xi_old = Xi.copy()\n",
    "        for k in range(d):\n",
    "            small = np.abs(Xi[:, k]) < lam\n",
    "            Xi[small, k] = 0.0\n",
    "            big_idx = ~small\n",
    "            if np.any(big_idx):\n",
    "                Xi[big_idx, k], _, _, _ = lstsq(Theta[:, big_idx], dXdt[:, k], rcond=None)\n",
    "        if np.max(np.abs(Xi - Xi_old)) < tol:\n",
    "            break\n",
    "    return Xi\n",
    "\n",
    "lam = 0.1  # tune this\n",
    "Xi_scaled = stlsq(Theta, dXdt, lam=lam, max_iter=25)\n",
    "# unscale coefficients back to the raw (un-normalized) library\n",
    "Xi = Xi_scaled / col_scales[:, None]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8831e57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx/dt = -5.56505·1 -16.97208·x1 +13.63118·x2 +1.01893·x3 +0.19516·x1^2 -0.19378·x1*x2 +0.36958·x1*x3 +0.04524·x2^2 -0.28345·x2*x3 -0.03504·x3^2\n",
      "dy/dt = -0.42397·1 -7.65496·x1 +7.53936·x2 +0.54072·x3 +0.15682·x1^2 -0.13100·x1*x2 +0.07683·x1*x3 +0.01465·x2^2 -0.12469·x2*x3 -0.02413·x3^2\n",
      "dz/dt = +49.59975·1 -2.17455·x1 +1.51900·x2 -5.17219·x3 +0.88247·x1^2 -0.88572·x1*x2 +0.05685·x1*x3 +0.52385·x2^2 -0.04225·x2*x3 +0.04936·x3^2\n"
     ]
    }
   ],
   "source": [
    "def print_model(Xi, feat_names, var_names=(\"x\",\"y\",\"z\"), tol=1e-10):\n",
    "    p, d = Xi.shape\n",
    "    for k, vk in enumerate(var_names, start=1):\n",
    "        terms = []\n",
    "        for j in range(p):\n",
    "            c = Xi[j, k-1]\n",
    "            if abs(c) > tol:\n",
    "                terms.append(f\"{c:+.5f}·{feat_names[j]}\")\n",
    "        rhs = \" \".join(terms) if terms else \"0\"\n",
    "        print(f\"d{vk}/dt = {rhs}\")\n",
    "\n",
    "print_model(Xi, feat_names, var_names=(\"x\",\"y\",\"z\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7bf82903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma ≈ 13.6312, 16.9721 | rho ≈ -7.6550 | beta ≈ 5.1722\n",
      "coeff(xz in dy) ≈ -0.0768 | coeff(xy in dz) ≈ -0.8857\n"
     ]
    }
   ],
   "source": [
    "def coeff(feat, eq):\n",
    "    j = feat_names.index(feat)\n",
    "    return Xi[j, eq]\n",
    "\n",
    "sigma_hat =  coeff(\"x2\", 0)    # y in dx/dt\n",
    "sigma_hat2 = -coeff(\"x1\", 0)   # -x in dx/dt (should match sigma)\n",
    "rho_hat   =  coeff(\"x1\", 1)    # x in dy/dt\n",
    "beta_hat  = -coeff(\"x3\", 2)    # -z in dz/dt\n",
    "xz_hat    = -coeff(\"x1*x3\",1)  # -xz in dy/dt (should be -1)\n",
    "xy_hat    =  coeff(\"x1*x2\",2)  #  xy in dz/dt (should be +1)\n",
    "\n",
    "print(f\"sigma ≈ {sigma_hat:.4f}, {sigma_hat2:.4f} | rho ≈ {rho_hat:.4f} | beta ≈ {beta_hat:.4f}\")\n",
    "print(f\"coeff(xz in dy) ≈ {xz_hat:.4f} | coeff(xy in dz) ≈ {xy_hat:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca175b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a09c2a2f",
   "metadata": {},
   "source": [
    "$$ \\text{sanity check against reference library} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ee90a95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!pip install pysindy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpysindy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mps\u001b[39;00m\n\u001b[1;32m      3\u001b[0m opt \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mSTLSQ(threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      4\u001b[0m feat \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mPolynomialLibrary(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, include_interaction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, include_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pysindy/__init__.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m differentiation\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_library\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizers\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pysindy/differentiation/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseDifferentiation\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfinite_difference\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FiniteDifference\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msindy_derivative\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SINDyDerivative\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pysindy/differentiation/base.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mBase class for numerical differentiation methods\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mabc\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEstimator\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseDifferentiation\u001b[39;00m(BaseEstimator):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Base class for differentiation methods.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m            that value here.  Methods that do not simply save x here.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#!pip install pysindy\n",
    "import pysindy as ps\n",
    "opt = ps.STLSQ(threshold=0.1)\n",
    "feat = ps.PolynomialLibrary(degree=2, include_interaction=True, include_bias=True)\n",
    "model = ps.SINDy(optimizer=opt, feature_library=feat)\n",
    "model.fit(X_data, t=dt)\n",
    "model.print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f9f9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
